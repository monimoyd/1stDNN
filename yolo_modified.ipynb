{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo_modified.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monimoyd/1stDNN/blob/master/yolo_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lo3JbAMnJWMd",
        "colab_type": "code",
        "outputId": "e0d0984f-b2a7-4d36-f168-f7e17a167e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1024\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "num_layer = 5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l1fSx-2gJl2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atgYIh18JsMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def space_to_depth_x8(x):\n",
        "    return tf.space_to_depth(x, block_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6qj4_LNPxvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a block using 32 (same) > 64 (same) > 128 (same) > 256 (same) > 512 (same) > MP \n",
        "def create_block(input, block_no, num_layer = 5):\n",
        "    temp = input\n",
        "    layer_name = \"block-\" + str(block_no) + \"_bc\"\n",
        "    BatchNorm = BatchNormalization(name=layer_name)(temp)\n",
        "    temp = Activation('relu')(BatchNorm)\n",
        "    num_filter = 32\n",
        "    one_x_one_filter_id = 1\n",
        "    # Adding layers \n",
        "    for i in range(num_layer):  \n",
        "        # Adding 1x1 filter to reduce number of channels otherwise number of parameters crosses million\n",
        "        # and processing becomes very slow\n",
        "        if (num_filter >= 128) :\n",
        "            layer_name = \"block-\" + str(block_no) + \"_1_1-\" + str(one_x_one_filter_id)\n",
        "            one_x_one_filter_id += 1\n",
        "            temp = Conv2D(16, 1, activation='relu', name=layer_name, use_bias=False)(temp)\n",
        "        layer_name = \"block-\" + str(block_no) + \"_layer\" + \"-\" + str(i +1)\n",
        "        temp = Conv2D(num_filter, (3,3), name=layer_name, use_bias=False, padding='same', activation='relu')(temp)\n",
        "        num_filter *= 2\n",
        "        \n",
        "    # Adding 1x1 filter to reduce number of channels before applying Maxpool\n",
        "    layer_name = \"block-\" + str(block_no) + \"_1_1-\" + str(one_x_one_filter_id)\n",
        "    one_x_one_filter_id += 1\n",
        "    temp = Conv2D(16, 1, activation='relu', name=layer_name, use_bias=False)(temp)\n",
        "    \n",
        "    # Adding maxpool\n",
        "    layer_name = \"block-\" + str(block_no) + \"_maxpool_layer\"\n",
        "    temp = MaxPooling2D(pool_size=(2, 2), name=layer_name)(temp)    \n",
        "    \n",
        "    return temp\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZZxo3EnEJyWW",
        "colab_type": "code",
        "outputId": "d13c090e-4163-4340-e1a2-bf9a6bf2d59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "# Layer 1\n",
        "layer1 = create_block(input, 1, num_layer)\n",
        "#layer1 = Conv2D(32, 1, activation='relu', name=\"block1_1_1\", use_bias=False)(layer1)\n",
        "skip_connection = layer1\n",
        "\n",
        "\n",
        "# Layer 2\n",
        "layer2 = create_block(layer1, 2, num_layer)\n",
        "#layer2 = Conv2D(32, (1, 1), activation='relu', name=\"block2_1_1\", use_bias=False)(layer2)\n",
        "\n",
        "# Layer 3\n",
        "layer3 = create_block(layer2, 3,num_layer)\n",
        "#layer3 = Conv2D(32, (1, 1), activation='relu', name=\"block3_1_1\", use_bias=False)(layer3)\n",
        "\n",
        "# Layer 4\n",
        "layer4 = create_block(layer3, 4,num_layer)\n",
        "#layer4 = Conv2D(32, (1, 1), activation='relu', name=\"block4_1_1\", use_bias=False)(layer4)\n",
        "\n",
        "# skip_connection: prepare for concatenation by applying 1x1 filter and Lambda on space_to_depth_x8 \n",
        "skip_connection = BatchNormalization(name=\"block-5_bc\")(skip_connection)\n",
        "skip_connection = ReLU()(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x8)(skip_connection)\n",
        "\n",
        "# Concatenate layer4 and skip_connection to get previous output layer and then flatten\n",
        "\n",
        "prev_output_layer = Concatenate(axis=-1)([layer4,skip_connection])\n",
        "prev_output_layer = Flatten()(prev_output_layer)\n",
        "\n",
        "# Output layer\n",
        "\n",
        "output = Dense(num_classes, activation='softmax')(prev_output_layer)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kl5zDGcnOL03",
        "colab_type": "code",
        "outputId": "c1e385ae-9cd3-4519-eb02-e011085bd5f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2167
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block-1_bc (BatchNormalization) (None, 32, 32, 3)    12          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 3)    0           block-1_bc[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block-1_layer-1 (Conv2D)        (None, 32, 32, 32)   864         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block-1_layer-2 (Conv2D)        (None, 32, 32, 64)   18432       block-1_layer-1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-1_1_1-1 (Conv2D)          (None, 32, 32, 16)   1024        block-1_layer-2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-1_layer-3 (Conv2D)        (None, 32, 32, 128)  18432       block-1_1_1-1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-1_1_1-2 (Conv2D)          (None, 32, 32, 16)   2048        block-1_layer-3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-1_layer-4 (Conv2D)        (None, 32, 32, 256)  36864       block-1_1_1-2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-1_1_1-3 (Conv2D)          (None, 32, 32, 16)   4096        block-1_layer-4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-1_layer-5 (Conv2D)        (None, 32, 32, 512)  73728       block-1_1_1-3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-1_1_1-4 (Conv2D)          (None, 32, 32, 16)   8192        block-1_layer-5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-1_maxpool_layer (MaxPooli (None, 16, 16, 16)   0           block-1_1_1-4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-2_bc (BatchNormalization) (None, 16, 16, 16)   64          block-1_maxpool_layer[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 16)   0           block-2_bc[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block-2_layer-1 (Conv2D)        (None, 16, 16, 32)   4608        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block-2_layer-2 (Conv2D)        (None, 16, 16, 64)   18432       block-2_layer-1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-2_1_1-1 (Conv2D)          (None, 16, 16, 16)   1024        block-2_layer-2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-2_layer-3 (Conv2D)        (None, 16, 16, 128)  18432       block-2_1_1-1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-2_1_1-2 (Conv2D)          (None, 16, 16, 16)   2048        block-2_layer-3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-2_layer-4 (Conv2D)        (None, 16, 16, 256)  36864       block-2_1_1-2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-2_1_1-3 (Conv2D)          (None, 16, 16, 16)   4096        block-2_layer-4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-2_layer-5 (Conv2D)        (None, 16, 16, 512)  73728       block-2_1_1-3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-2_1_1-4 (Conv2D)          (None, 16, 16, 16)   8192        block-2_layer-5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-2_maxpool_layer (MaxPooli (None, 8, 8, 16)     0           block-2_1_1-4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-3_bc (BatchNormalization) (None, 8, 8, 16)     64          block-2_maxpool_layer[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 8, 8, 16)     0           block-3_bc[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block-3_layer-1 (Conv2D)        (None, 8, 8, 32)     4608        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block-3_layer-2 (Conv2D)        (None, 8, 8, 64)     18432       block-3_layer-1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-3_1_1-1 (Conv2D)          (None, 8, 8, 16)     1024        block-3_layer-2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-3_layer-3 (Conv2D)        (None, 8, 8, 128)    18432       block-3_1_1-1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-3_1_1-2 (Conv2D)          (None, 8, 8, 16)     2048        block-3_layer-3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-3_layer-4 (Conv2D)        (None, 8, 8, 256)    36864       block-3_1_1-2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-3_1_1-3 (Conv2D)          (None, 8, 8, 16)     4096        block-3_layer-4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-3_layer-5 (Conv2D)        (None, 8, 8, 512)    73728       block-3_1_1-3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-3_1_1-4 (Conv2D)          (None, 8, 8, 16)     8192        block-3_layer-5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-3_maxpool_layer (MaxPooli (None, 4, 4, 16)     0           block-3_1_1-4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-4_bc (BatchNormalization) (None, 4, 4, 16)     64          block-3_maxpool_layer[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 4, 4, 16)     0           block-4_bc[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block-4_layer-1 (Conv2D)        (None, 4, 4, 32)     4608        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block-4_layer-2 (Conv2D)        (None, 4, 4, 64)     18432       block-4_layer-1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-4_1_1-1 (Conv2D)          (None, 4, 4, 16)     1024        block-4_layer-2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-4_layer-3 (Conv2D)        (None, 4, 4, 128)    18432       block-4_1_1-1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-4_1_1-2 (Conv2D)          (None, 4, 4, 16)     2048        block-4_layer-3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-4_layer-4 (Conv2D)        (None, 4, 4, 256)    36864       block-4_1_1-2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-4_1_1-3 (Conv2D)          (None, 4, 4, 16)     4096        block-4_layer-4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block-4_layer-5 (Conv2D)        (None, 4, 4, 512)    73728       block-4_1_1-3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block-5_bc (BatchNormalization) (None, 16, 16, 16)   64          block-1_maxpool_layer[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block-4_1_1-4 (Conv2D)          (None, 4, 4, 16)     8192        block-4_layer-5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 16, 16, 16)   0           block-5_bc[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block-4_maxpool_layer (MaxPooli (None, 2, 2, 16)     0           block-4_1_1-4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 2, 2, 1024)   0           re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2, 2, 1040)   0           block-4_maxpool_layer[0][0]      \n",
            "                                                                 lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 4160)         0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           41610       flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 707,830\n",
            "Trainable params: 707,696\n",
            "Non-trainable params: 134\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bd5xQbFyOTkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VeSW5FhD3XJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "16faa193-83f9-48ec-bd06-5c16750d4e2b"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 1929s 39ms/step - loss: 1.9059 - acc: 0.3114 - val_loss: 1.7502 - val_acc: 0.3919\n",
            "Epoch 2/50\n",
            "21504/50000 [===========>..................] - ETA: 17:13 - loss: 1.5642 - acc: 0.4414"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}