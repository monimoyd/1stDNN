{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lo3JbAMnJWMd",
    "outputId": "e0d0984f-b2a7-4d36-f168-f7e17a167e48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "import keras\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import ReLU\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "num_layer = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1fSx-2gJl2a"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atgYIh18JsMK"
   },
   "outputs": [],
   "source": [
    "def space_to_depth_x8(x):\n",
    "    return tf.space_to_depth(x, block_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6qj4_LNPxvA"
   },
   "outputs": [],
   "source": [
    "# Create a block using 32 (same) > 64 (same) > 128 (same) > 256 (same) > 512 (same) > MP \n",
    "def create_block(input, block_no, num_layer = 5):\n",
    "    temp = input\n",
    "    layer_name = \"block-\" + str(block_no) + \"_bc\"\n",
    "    BatchNorm = BatchNormalization(name=layer_name)(temp)\n",
    "    temp = Activation('relu')(BatchNorm)\n",
    "    num_filter = 32\n",
    "    one_x_one_filter_id = 1\n",
    "    # Adding layers \n",
    "    for i in range(num_layer):  \n",
    "        # Adding 1x1 filter to reduce number of channels otherwise number of parameters crosses million\n",
    "        # and processing becomes very slow\n",
    "        if (num_filter >= 128) :\n",
    "            layer_name = \"block-\" + str(block_no) + \"_1_1-\" + str(one_x_one_filter_id)\n",
    "            one_x_one_filter_id += 1\n",
    "            temp = Conv2D(32, 1, activation='relu', name=layer_name, use_bias=False)(temp)\n",
    "        layer_name = \"block-\" + str(block_no) + \"_layer\" + \"-\" + str(i +1)\n",
    "        temp = Conv2D(num_filter, (3,3), name=layer_name, use_bias=False, padding='same', activation='relu')(temp)\n",
    "        num_filter *= 2\n",
    "        \n",
    "    # Adding 1x1 filter to reduce number of channels before applying Maxpool\n",
    "    layer_name = \"block-\" + str(block_no) + \"_1_1-\" + str(one_x_one_filter_id)\n",
    "    one_x_one_filter_id += 1\n",
    "    temp = Conv2D(32, 1, activation='relu', name=layer_name, use_bias=False)(temp)\n",
    "    \n",
    "    # Adding maxpool\n",
    "    layer_name = \"block-\" + str(block_no) + \"_maxpool_layer\"\n",
    "    temp = MaxPooling2D(pool_size=(2, 2), name=layer_name)(temp)    \n",
    "    \n",
    "    return temp\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "ZZxo3EnEJyWW",
    "outputId": "d13c090e-4163-4340-e1a2-bf9a6bf2d59a"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "\n",
    "# Layer 1\n",
    "layer1 = create_block(input, 1, num_layer)\n",
    "skip_connection = layer1\n",
    "\n",
    "\n",
    "# Layer 2\n",
    "layer2 = create_block(layer1, 2, num_layer)\n",
    "\n",
    "# Layer 3\n",
    "layer3 = create_block(layer2, 3,num_layer)\n",
    "\n",
    "# Layer 4\n",
    "layer4 = create_block(layer3, 4,num_layer)\n",
    "\n",
    "# skip_connection: prepare for concatenation by applying 1x1 filter and Lambda on space_to_depth_x8 \n",
    "skip_connection = BatchNormalization(name=\"block-5_bc\")(skip_connection)\n",
    "skip_connection = ReLU()(skip_connection)\n",
    "skip_connection = Lambda(space_to_depth_x8)(skip_connection)\n",
    "\n",
    "# Concatenate layer4 and skip_connection to get previous output layer and then flatten\n",
    "\n",
    "prev_output_layer = Concatenate(axis=-1)([layer4,skip_connection])\n",
    "prev_output_layer = Flatten()(prev_output_layer)\n",
    "\n",
    "# Output layer\n",
    "\n",
    "output = Dense(num_classes, activation='softmax')(prev_output_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2167
    },
    "colab_type": "code",
    "id": "Kl5zDGcnOL03",
    "outputId": "c1e385ae-9cd3-4519-eb02-e011085bd5f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block-1_bc (BatchNormalization) (None, 32, 32, 3)    12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 3)    0           block-1_bc[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block-1_layer-1 (Conv2D)        (None, 32, 32, 32)   864         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block-1_layer-2 (Conv2D)        (None, 32, 32, 64)   18432       block-1_layer-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-1_1_1-1 (Conv2D)          (None, 32, 32, 32)   2048        block-1_layer-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-1_layer-3 (Conv2D)        (None, 32, 32, 128)  36864       block-1_1_1-1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-1_1_1-2 (Conv2D)          (None, 32, 32, 32)   4096        block-1_layer-3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-1_layer-4 (Conv2D)        (None, 32, 32, 256)  73728       block-1_1_1-2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-1_1_1-3 (Conv2D)          (None, 32, 32, 32)   8192        block-1_layer-4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-1_layer-5 (Conv2D)        (None, 32, 32, 512)  147456      block-1_1_1-3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-1_1_1-4 (Conv2D)          (None, 32, 32, 32)   16384       block-1_layer-5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-1_maxpool_layer (MaxPooli (None, 16, 16, 32)   0           block-1_1_1-4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-2_bc (BatchNormalization) (None, 16, 16, 32)   128         block-1_maxpool_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 32)   0           block-2_bc[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block-2_layer-1 (Conv2D)        (None, 16, 16, 32)   9216        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block-2_layer-2 (Conv2D)        (None, 16, 16, 64)   18432       block-2_layer-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-2_1_1-1 (Conv2D)          (None, 16, 16, 32)   2048        block-2_layer-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-2_layer-3 (Conv2D)        (None, 16, 16, 128)  36864       block-2_1_1-1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-2_1_1-2 (Conv2D)          (None, 16, 16, 32)   4096        block-2_layer-3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-2_layer-4 (Conv2D)        (None, 16, 16, 256)  73728       block-2_1_1-2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-2_1_1-3 (Conv2D)          (None, 16, 16, 32)   8192        block-2_layer-4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-2_layer-5 (Conv2D)        (None, 16, 16, 512)  147456      block-2_1_1-3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-2_1_1-4 (Conv2D)          (None, 16, 16, 32)   16384       block-2_layer-5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-2_maxpool_layer (MaxPooli (None, 8, 8, 32)     0           block-2_1_1-4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-3_bc (BatchNormalization) (None, 8, 8, 32)     128         block-2_maxpool_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 32)     0           block-3_bc[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block-3_layer-1 (Conv2D)        (None, 8, 8, 32)     9216        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block-3_layer-2 (Conv2D)        (None, 8, 8, 64)     18432       block-3_layer-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-3_1_1-1 (Conv2D)          (None, 8, 8, 32)     2048        block-3_layer-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-3_layer-3 (Conv2D)        (None, 8, 8, 128)    36864       block-3_1_1-1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-3_1_1-2 (Conv2D)          (None, 8, 8, 32)     4096        block-3_layer-3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-3_layer-4 (Conv2D)        (None, 8, 8, 256)    73728       block-3_1_1-2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-3_1_1-3 (Conv2D)          (None, 8, 8, 32)     8192        block-3_layer-4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-3_layer-5 (Conv2D)        (None, 8, 8, 512)    147456      block-3_1_1-3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-3_1_1-4 (Conv2D)          (None, 8, 8, 32)     16384       block-3_layer-5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-3_maxpool_layer (MaxPooli (None, 4, 4, 32)     0           block-3_1_1-4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-4_bc (BatchNormalization) (None, 4, 4, 32)     128         block-3_maxpool_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 4, 4, 32)     0           block-4_bc[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block-4_layer-1 (Conv2D)        (None, 4, 4, 32)     9216        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block-4_layer-2 (Conv2D)        (None, 4, 4, 64)     18432       block-4_layer-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-4_1_1-1 (Conv2D)          (None, 4, 4, 32)     2048        block-4_layer-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-4_layer-3 (Conv2D)        (None, 4, 4, 128)    36864       block-4_1_1-1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-4_1_1-2 (Conv2D)          (None, 4, 4, 32)     4096        block-4_layer-3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-4_layer-4 (Conv2D)        (None, 4, 4, 256)    73728       block-4_1_1-2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-4_1_1-3 (Conv2D)          (None, 4, 4, 32)     8192        block-4_layer-4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block-4_layer-5 (Conv2D)        (None, 4, 4, 512)    147456      block-4_1_1-3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block-5_bc (BatchNormalization) (None, 16, 16, 32)   128         block-1_maxpool_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block-4_1_1-4 (Conv2D)          (None, 4, 4, 32)     16384       block-4_layer-5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 16, 16, 32)   0           block-5_bc[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block-4_maxpool_layer (MaxPooli (None, 2, 2, 32)     0           block-4_1_1-4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2, 2, 2048)   0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 2, 2080)   0           block-4_maxpool_layer[0][0]      \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8320)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           83210       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,341,046\n",
      "Trainable params: 1,340,784\n",
      "Non-trainable params: 262\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bd5xQbFyOTkj"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "VeSW5FhD3XJO",
    "outputId": "16faa193-83f9-48ec-bd06-5c16750d4e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 97s 2ms/step - loss: 1.5158 - acc: 0.4656 - val_loss: 1.2432 - val_acc: 0.5674\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1.1259 - acc: 0.6094 - val_loss: 1.1173 - val_acc: 0.6127\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.9697 - acc: 0.6655 - val_loss: 1.1831 - val_acc: 0.6157\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.8752 - acc: 0.6986 - val_loss: 1.1363 - val_acc: 0.6467\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 85s 2ms/step - loss: 0.8030 - acc: 0.7218 - val_loss: 0.9813 - val_acc: 0.6603\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.7425 - acc: 0.7430 - val_loss: 0.9464 - val_acc: 0.6880\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.6784 - acc: 0.7628 - val_loss: 1.0170 - val_acc: 0.6624\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.6270 - acc: 0.7812 - val_loss: 0.9404 - val_acc: 0.6925\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.5770 - acc: 0.7989 - val_loss: 1.0801 - val_acc: 0.6671\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.5310 - acc: 0.8134 - val_loss: 1.1070 - val_acc: 0.6750\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.4950 - acc: 0.8254 - val_loss: 1.1242 - val_acc: 0.6676\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.4523 - acc: 0.8403 - val_loss: 1.1378 - val_acc: 0.6758\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.4182 - acc: 0.8514 - val_loss: 1.2851 - val_acc: 0.6668\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.3859 - acc: 0.8633 - val_loss: 1.3236 - val_acc: 0.6620\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.3570 - acc: 0.8726 - val_loss: 1.3994 - val_acc: 0.6640\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.3328 - acc: 0.8823 - val_loss: 1.4880 - val_acc: 0.6559\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.3011 - acc: 0.8941 - val_loss: 1.4288 - val_acc: 0.6656\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.2776 - acc: 0.9004 - val_loss: 1.5779 - val_acc: 0.6652\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.2597 - acc: 0.9082 - val_loss: 1.5923 - val_acc: 0.6602\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.2373 - acc: 0.9139 - val_loss: 1.6558 - val_acc: 0.6568\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.2199 - acc: 0.9212 - val_loss: 1.6209 - val_acc: 0.6637\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.2054 - acc: 0.9267 - val_loss: 1.6945 - val_acc: 0.6623\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.1871 - acc: 0.9330 - val_loss: 1.7885 - val_acc: 0.6594\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.1772 - acc: 0.9359 - val_loss: 1.8127 - val_acc: 0.6541\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.1719 - acc: 0.9384 - val_loss: 1.8457 - val_acc: 0.6615\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 0.1588 - acc: 0.9434 - val_loss: 1.8563 - val_acc: 0.6601\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.1536 - acc: 0.9456 - val_loss: 1.9014 - val_acc: 0.6677\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.1422 - acc: 0.9492 - val_loss: 1.9086 - val_acc: 0.6680\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 0.1354 - acc: 0.9523 - val_loss: 1.9856 - val_acc: 0.6655\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 0.1299 - acc: 0.9540 - val_loss: 1.9882 - val_acc: 0.6543\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 0.1146 - acc: 0.9604 - val_loss: 1.9191 - val_acc: 0.6686\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 0.1158 - acc: 0.9593 - val_loss: 1.9723 - val_acc: 0.6715\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.1183 - acc: 0.9579 - val_loss: 2.0937 - val_acc: 0.6673\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.1107 - acc: 0.9620 - val_loss: 2.1779 - val_acc: 0.6676\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0986 - acc: 0.9649 - val_loss: 2.1593 - val_acc: 0.6587\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0957 - acc: 0.9663 - val_loss: 2.2610 - val_acc: 0.6553\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0931 - acc: 0.9680 - val_loss: 2.1478 - val_acc: 0.6666\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 0.0924 - acc: 0.9677 - val_loss: 2.2947 - val_acc: 0.6605\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 89s 2ms/step - loss: 0.0866 - acc: 0.9692 - val_loss: 2.3115 - val_acc: 0.6673\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 89s 2ms/step - loss: 0.0880 - acc: 0.9701 - val_loss: 2.4095 - val_acc: 0.6489\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 0.0794 - acc: 0.9720 - val_loss: 2.1113 - val_acc: 0.6714\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0813 - acc: 0.9714 - val_loss: 2.2698 - val_acc: 0.6658\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0811 - acc: 0.9724 - val_loss: 2.3517 - val_acc: 0.6665\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0752 - acc: 0.9734 - val_loss: 2.5763 - val_acc: 0.6524\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0710 - acc: 0.9755 - val_loss: 2.4332 - val_acc: 0.6535\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0783 - acc: 0.9726 - val_loss: 2.4452 - val_acc: 0.6684\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0672 - acc: 0.9769 - val_loss: 2.2891 - val_acc: 0.6751\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0651 - acc: 0.9770 - val_loss: 2.3925 - val_acc: 0.6634\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 87s 2ms/step - loss: 0.0743 - acc: 0.9743 - val_loss: 2.4577 - val_acc: 0.6703\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 90s 2ms/step - loss: 0.0607 - acc: 0.9788 - val_loss: 2.3833 - val_acc: 0.6675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e218fcdd8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "yolo_modified.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
